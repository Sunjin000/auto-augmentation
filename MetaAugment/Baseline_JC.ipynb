{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_ZJ2LqDiu_v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ksS_duLFADW"
      },
      "outputs": [],
      "source": [
        "\"\"\"Define internal NN module that trains on the dataset\"\"\"\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, img_height, img_width, num_labels, img_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(img_channels, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(int((((img_height-4)/2-4)/2)*(((img_width-4)/2-4)/2)*16), 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, num_labels)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define internal NN module that trains on the dataset\"\"\"\n",
        "class EasyNet(nn.Module):\n",
        "    def __init__(self, img_height, img_width, num_labels, img_channels):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(img_height*img_width*img_channels, 2048)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(2048, num_labels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.view(x.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu2(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "ukf2-C94UWzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define internal NN module that trains on the dataset\"\"\"\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, img_height, img_width, num_labels, img_channels):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(img_height*img_width*img_channels, num_labels)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.view(x.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu1(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "Fd9_36R3zx5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xujQtvVWBgMH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Make toy dataset\"\"\"\n",
        "\n",
        "def create_toy(train_dataset, test_dataset, batch_size, n_samples):\n",
        "    \n",
        "    # shuffle and take first n_samples %age of training dataset\n",
        "    shuffle_order_train = np.random.RandomState(seed=100).permutation(len(train_dataset))\n",
        "    shuffled_train_dataset = torch.utils.data.Subset(train_dataset, shuffle_order_train)\n",
        "    indices_train = torch.arange(int(n_samples*len(train_dataset)))\n",
        "    reduced_train_dataset = data_utils.Subset(shuffled_train_dataset, indices_train)\n",
        "\n",
        "    # shuffle and take first n_samples %age of test dataset\n",
        "    shuffle_order_test = np.random.RandomState(seed=1000).permutation(len(test_dataset))\n",
        "    shuffled_test_dataset = torch.utils.data.Subset(test_dataset, shuffle_order_test)\n",
        "    indices_test = torch.arange(int(n_samples*len(test_dataset)))\n",
        "    reduced_test_dataset = data_utils.Subset(shuffled_test_dataset, indices_test)\n",
        "\n",
        "    # push into DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(reduced_train_dataset, batch_size=batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(reduced_test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu_4I4qkbx73"
      },
      "outputs": [],
      "source": [
        "def run_baseline(batch_size=32, learning_rate=1e-1, ds=\"MNIST\", toy_size=0.02, max_epochs=100, early_stop_num=10, early_stop_flag=True, average_validation=[15,25], IsLeNet=\"LeNet\"):\n",
        "\n",
        "    # create transformations using above info\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()])\n",
        "\n",
        "    # open data and apply these transformations\n",
        "    if ds == \"MNIST\":\n",
        "        train_dataset = datasets.MNIST(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.MNIST(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "    elif ds == \"KMNIST\":\n",
        "        train_dataset = datasets.KMNIST(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.KMNIST(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "    elif ds == \"FashionMNIST\":\n",
        "        train_dataset = datasets.FashionMNIST(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.FashionMNIST(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "    elif ds == \"CIFAR10\":\n",
        "        train_dataset = datasets.CIFAR10(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.CIFAR10(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "    elif ds == \"CIFAR100\":\n",
        "        train_dataset = datasets.CIFAR100(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.CIFAR100(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "\n",
        "    # check sizes of images\n",
        "    img_height = len(train_dataset[0][0][0])\n",
        "    img_width = len(train_dataset[0][0][0][0])\n",
        "    img_channels = len(train_dataset[0][0])\n",
        "\n",
        "    # check output labels\n",
        "    if ds == \"CIFAR10\" or ds == \"CIFAR100\":\n",
        "        num_labels = (max(train_dataset.targets) - min(train_dataset.targets) + 1)\n",
        "    else:\n",
        "        num_labels = (max(train_dataset.targets) - min(train_dataset.targets) + 1).item()\n",
        "\n",
        "    # create toy dataset from above uploaded data\n",
        "    train_loader, test_loader = create_toy(train_dataset, test_dataset, batch_size, toy_size)\n",
        "\n",
        "    # create model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    if IsLeNet == \"LeNet\":\n",
        "        model = LeNet(img_height, img_width, num_labels, img_channels).to(device) # added .to(device)\n",
        "    elif IsLeNet == \"EasyNet\":\n",
        "        model = EasyNet(img_height, img_width, num_labels, img_channels).to(device) # added .to(device)\n",
        "    else:\n",
        "        model = SimpleNet(img_height, img_width, num_labels, img_channels).to(device) # added .to(device)\n",
        "    sgd = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    # set variables for best validation accuracy and early stop count\n",
        "    best_acc = 0\n",
        "    early_stop_cnt = 0\n",
        "    total_val = 0\n",
        "\n",
        "    # train model and check validation accuracy each epoch\n",
        "    for _epoch in range(max_epochs):\n",
        "\n",
        "        # train model\n",
        "        model.train()\n",
        "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
        "            train_x, train_label = train_x.to(device), train_label.to(device) # new code\n",
        "            label_np = np.zeros((train_label.shape[0], num_labels))\n",
        "            sgd.zero_grad()\n",
        "            predict_y = model(train_x.float())\n",
        "            loss = cost(predict_y, train_label.long())\n",
        "            loss.backward()\n",
        "            sgd.step()\n",
        "\n",
        "        # check validation accuracy on validation set\n",
        "        correct = 0\n",
        "        _sum = 0\n",
        "        model.eval()\n",
        "        for idx, (test_x, test_label) in enumerate(test_loader):\n",
        "            test_x, test_label = test_x.to(device), test_label.to(device) # new code \n",
        "            predict_y = model(test_x.float()).detach()\n",
        "            #predict_ys = np.argmax(predict_y, axis=-1)\n",
        "            predict_ys = torch.argmax(predict_y, axis=-1) # changed np to torch\n",
        "            #label_np = test_label.numpy()\n",
        "            _ = predict_ys == test_label\n",
        "            #correct += np.sum(_.numpy(), axis=-1)\n",
        "            correct += np.sum(_.cpu().numpy(), axis=-1) # added .cpu()\n",
        "            _sum += _.shape[0]\n",
        "\n",
        "        acc = correct / _sum\n",
        "\n",
        "        # update the total validation\n",
        "        if average_validation[0] <= _epoch <= average_validation[1]:\n",
        "            total_val += acc\n",
        "\n",
        "        # update best validation accuracy if it was higher, otherwise increase early stop count\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        # exit if validation gets worse over 10 runs and using early stopping\n",
        "        if early_stop_cnt >= early_stop_num and early_stop_flag:\n",
        "            return best_acc\n",
        "\n",
        "        # exit if using fixed epoch length\n",
        "        if _epoch >= average_validation[1] and not early_stop_flag:\n",
        "            return total_val / (average_validation[1] - average_validation[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 32               # size of batch the inner NN is trained with\n",
        "learning_rate = 1e-1          # fix learning rate\n",
        "ds = \"CIFAR10\"               # pick dataset (MNIST, KMNIST, FashionMNIST, CIFAR10, CIFAR100)\n",
        "toy_size = 1               # total propeortion of training and test set we use\n",
        "max_epochs = 100              # max number of epochs that is run if early stopping is not hit\n",
        "early_stop_num = 10           # max number of worse validation scores before early stopping is triggered\n",
        "early_stop_flag = True        # implement early stopping or not\n",
        "average_validation = [15,25]  # if not implementing early stopping, what epochs are we averaging over\n",
        "num_iterations = 10            # how many iterations are we averaging over\n",
        "IsLeNet = \"LeNet\"             # using LeNet or EasyNet or SimpleNet\n",
        "\n",
        "# run using early stopping\n",
        "best_accuracies_1 = []\n",
        "for baselines in trange(num_iterations):\n",
        "    best_acc = run_baseline(batch_size, learning_rate, ds, toy_size, max_epochs, early_stop_num, early_stop_flag, average_validation, IsLeNet)\n",
        "    best_accuracies_1.append(best_acc)\n",
        "    if baselines % 10 == 0:\n",
        "        print(\"{}\\tBest accuracy: {:.2f}%\".format(baselines, best_acc*100))\n",
        "print(\"Average best accuracy: {:.2f}%\\n\".format(np.mean(best_accuracies_1)*100))\n",
        "\n",
        "file = open(f\"{ds}_v1.txt\", \"w\")\n",
        "content = ','.join(str(e) for e in best_accuracies_1)\n",
        "file.write(content)\n",
        "file.close()\n",
        "\n",
        "# run using average validation losses\n",
        "early_stop_flag = False\n",
        "best_accuracies_2 = []\n",
        "for baselines in trange(num_iterations):\n",
        "    best_acc = run_baseline(batch_size, learning_rate, ds, toy_size, max_epochs, early_stop_num, early_stop_flag, average_validation, IsLeNet)\n",
        "    best_accuracies_2.append(best_acc)\n",
        "    if baselines % 10 == 0:\n",
        "        print(\"{}\\tBest accuracy: {:.2f}%\".format(baselines, best_acc*100))\n",
        "print(\"Average average accuracy: {:.2f}%\\n\".format(np.mean(best_accuracies_2)*100))\n",
        "\n",
        "file = open(f\"{ds}_v2.txt\", \"w\")\n",
        "content = ','.join(str(e) for e in best_accuracies_2)\n",
        "file.write(content)\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhYheLfBP33",
        "outputId": "4858df7d-ee29-4e3b-ac2f-e33a9a4bbf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [07:27<1:07:04, 447.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tBest accuracy: 54.32%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [12:32<48:32, 364.01s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [17:55<40:16, 345.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [23:17<33:35, 335.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [27:43<25:53, 310.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [32:27<20:06, 301.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [38:03<15:38, 312.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [46:02<12:11, 365.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [51:08<05:47, 347.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [57:26<00:00, 344.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average best accuracy: 55.38%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [07:50<1:10:35, 470.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tBest accuracy: 52.04%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [15:37<1:02:27, 468.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [23:15<54:05, 463.58s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [30:56<46:16, 462.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [38:38<38:31, 462.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [46:18<30:46, 461.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [53:57<23:02, 460.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [1:01:38<15:21, 460.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [1:09:19<07:40, 460.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [1:16:55<00:00, 461.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average average accuracy: 52.48%\n",
            "\n",
            "CPU times: user 2h 12min 17s, sys: 2min 26s, total: 2h 14min 43s\n",
            "Wall time: 2h 14min 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}