{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U_ZJ2LqDiu_v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4ksS_duLFADW"
      },
      "outputs": [],
      "source": [
        "\"\"\"Define internal NN module that trains on the dataset\"\"\"\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xujQtvVWBgMH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Make toy dataset\"\"\"\n",
        "\n",
        "def create_toy(train_dataset, test_dataset, batch_size, n_samples):\n",
        "    \n",
        "    # shuffle and take first n_samples %age of training dataset\n",
        "    shuffle_order_train = np.random.RandomState(seed=100).permutation(len(train_dataset))\n",
        "    shuffled_train_dataset = torch.utils.data.Subset(train_dataset, shuffle_order_train)\n",
        "    indices_train = torch.arange(int(n_samples*len(train_dataset)))\n",
        "    reduced_train_dataset = data_utils.Subset(shuffled_train_dataset, indices_train)\n",
        "\n",
        "    # shuffle and take first n_samples %age of test dataset\n",
        "    shuffle_order_test = np.random.RandomState(seed=1000).permutation(len(test_dataset))\n",
        "    shuffled_test_dataset = torch.utils.data.Subset(test_dataset, shuffle_order_test)\n",
        "    indices_test = torch.arange(int(n_samples*len(test_dataset)))\n",
        "    reduced_test_dataset = data_utils.Subset(shuffled_test_dataset, indices_test)\n",
        "\n",
        "    # push into DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(reduced_train_dataset, batch_size=batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(reduced_test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vu_4I4qkbx73"
      },
      "outputs": [],
      "source": [
        "def run_baseline(batch_size=32, toy_size=0.02, max_epochs=100, early_stop_num=10, early_stop_flag=True, average_validation=[15,25]):\n",
        "\n",
        "    # create transformations using above info\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()])\n",
        "\n",
        "    # open data and apply these transformations\n",
        "    train_dataset = datasets.MNIST(root='./MetaAugment/train', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./MetaAugment/test', train=False, download=True, transform=transform)\n",
        "\n",
        "    # create toy dataset from above uploaded data\n",
        "    train_loader, test_loader = create_toy(train_dataset, test_dataset, batch_size, toy_size)\n",
        "\n",
        "    # create model\n",
        "    model = LeNet()\n",
        "    sgd = optim.SGD(model.parameters(), lr=1e-1)\n",
        "    cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    # set variables for best validation accuracy and early stop count\n",
        "    best_acc = 0\n",
        "    early_stop_cnt = 0\n",
        "    total_val = 0\n",
        "\n",
        "    # train model and check validation accuracy each epoch\n",
        "    for _epoch in range(max_epochs):\n",
        "\n",
        "        # train model\n",
        "        model.train()\n",
        "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
        "            label_np = np.zeros((train_label.shape[0], 10))\n",
        "            sgd.zero_grad()\n",
        "            predict_y = model(train_x.float())\n",
        "            loss = cost(predict_y, train_label.long())\n",
        "            loss.backward()\n",
        "            sgd.step()\n",
        "\n",
        "        # check validation accuracy on validation set\n",
        "        correct = 0\n",
        "        _sum = 0\n",
        "        model.eval()\n",
        "        for idx, (test_x, test_label) in enumerate(test_loader):\n",
        "            predict_y = model(test_x.float()).detach()\n",
        "            predict_ys = np.argmax(predict_y, axis=-1)\n",
        "            label_np = test_label.numpy()\n",
        "            _ = predict_ys == test_label\n",
        "            correct += np.sum(_.numpy(), axis=-1)\n",
        "            _sum += _.shape[0]\n",
        "\n",
        "        acc = correct / _sum\n",
        "\n",
        "        # update the total validation\n",
        "        if average_validation[0] <= _epoch <= average_validation[1]:\n",
        "            total_val += acc\n",
        "\n",
        "        # update best validation accuracy if it was higher, otherwise increase early stop count\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        # exit if validation gets worse over 10 runs and using early stopping\n",
        "        if early_stop_cnt >= early_stop_num and early_stop_flag:\n",
        "            return best_acc\n",
        "\n",
        "        # exit if using fixed epoch length\n",
        "        if _epoch >= average_validation[1] and not early_stop_flag:\n",
        "            return total_val / (average_validation[1] - average_validation[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32               # size of batch the inner NN is trained with\n",
        "toy_size = 0.02               # total propeortion of training and test set we use\n",
        "max_epochs = 100              # max number of epochs that is run if early stopping is not hit\n",
        "early_stop_num = 10           # max number of worse validation scores before early stopping is triggered\n",
        "early_stop_flag = True        # implement early stopping or not\n",
        "average_validation = [15,25]  # if not implementing early stopping, what epochs are we averaging over\n",
        "num_iterations = 100          # how many iterations are we averaging over\n",
        "\n",
        "# run using early stopping\n",
        "best_accuracies = []\n",
        "for baselines in range(num_iterations):\n",
        "    best_acc = run_baseline(batch_size, toy_size, max_epochs, early_stop_num, early_stop_flag, average_validation)\n",
        "    best_accuracies.append(best_acc)\n",
        "    if baselines % 10 == 0:\n",
        "        print(\"{}\\tBest accuracy: {:.2f}%\".format(baselines, best_acc*100))\n",
        "print(\"Average best accuracy: {:.2f}%\\n\".format(np.mean(best_accuracies)*100))\n",
        "\n",
        "# run using average validation losses\n",
        "early_stop_flag = False\n",
        "best_accuracies = []\n",
        "for baselines in range(num_iterations):\n",
        "    best_acc = run_baseline(batch_size, toy_size, max_epochs, early_stop_num, early_stop_flag, average_validation)\n",
        "    best_accuracies.append(best_acc)\n",
        "    if baselines % 10 == 0:\n",
        "        print(\"{}\\tAverage accuracy: {:.2f}%\".format(baselines, best_acc*100))\n",
        "print(\"Average average accuracy: {:.2f}%\\n\".format(np.mean(best_accuracies)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhYheLfBP33",
        "outputId": "8009d87f-7e39-40e3-c6ef-8f3a12f9433f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tBest accuracy: 49.00%\n",
            "10\tBest accuracy: 86.50%\n",
            "20\tBest accuracy: 95.00%\n",
            "30\tBest accuracy: 54.00%\n",
            "40\tBest accuracy: 94.00%\n",
            "50\tBest accuracy: 93.50%\n",
            "60\tBest accuracy: 66.50%\n",
            "70\tBest accuracy: 94.50%\n",
            "80\tBest accuracy: 74.50%\n",
            "90\tBest accuracy: 74.00%\n",
            "Average best accuracy: 79.58%\n",
            "\n",
            "0\tAverage accuracy: 68.95%\n",
            "10\tAverage accuracy: 69.95%\n",
            "20\tAverage accuracy: 85.00%\n",
            "30\tAverage accuracy: 93.32%\n",
            "40\tAverage accuracy: 68.00%\n",
            "50\tAverage accuracy: 85.36%\n",
            "60\tAverage accuracy: 92.36%\n",
            "70\tAverage accuracy: 56.95%\n",
            "80\tAverage accuracy: 93.59%\n",
            "90\tAverage accuracy: 64.91%\n",
            "Average average accuracy: 78.90%\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}